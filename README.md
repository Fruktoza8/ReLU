# ReLU
проект создан в рамках лабораторной работы по дисциплине "Интеллектуальные системы и технологии"
обновлённый код прошлой версии нейросети

Функция активации: Relu
• Функция потерь: Cross-Entropy Loss
• Алгоритм изменения весов: обратное распространение ошибки с оптимизатором Adam
• Количество эпох обучения: при общей точности 95% требуется 100 эпох (во многом зависит от дата сета)
• Количество слоев в сети: 4, 2 из них – скрытые.
• Кол-во нейронов на скрытых слоях – 256, 128

сохранение весов работает неправильно, потом исправлю
